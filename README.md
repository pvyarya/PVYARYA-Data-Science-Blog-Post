# PVYARYA-Data-Science-Blog-Post
Motivation

The goal of this project is to walk through the CRISP-DM process to build and evaluate a predictive machine learning model, while also effectively communicating findings to stakeholders. The project highlights exploratory data analysis, model development, evaluation, and interpretation of results in a business-relevant context.

üìö Libraries Used

This project makes use of the following Python libraries:

pandas ‚Äì for data manipulation and cleaning

numpy ‚Äì for numerical operations

matplotlib / seaborn ‚Äì for visualizations in exploratory data analysis

scikit-learn ‚Äì for model building, evaluation, and metrics

jupyter notebook ‚Äì for running and presenting analyses interactively

üìÇ Files in This Repository

README.md ‚Üí Project overview, methodology, and results (this file)

data/ ‚Üí Dataset(s) used in the analysis (not included here for privacy reasons)

notebooks/Exploratory_Data_Analysis.ipynb ‚Üí Exploratory Data Analysis (EDA) with visualizations and summary statistics

notebooks/Data_Cleaning_and_Preprocessing.ipynb ‚Üí Data wrangling, missing values handling, and feature engineering

notebooks/Model_Training_and_Evaluation.ipynb ‚Üí Model training, evaluation (accuracy, recall, precision, F1, ROC/AUC)

reports/Results_Summary.pdf ‚Üí Summary of results and stakeholder-focused interpretation

üîé Key Steps (CRISP-DM Process)

Business Understanding

Define the prediction problem and determine its business value.

Data Understanding (Exploratory Data Analysis)

Generated distribution plots and histograms to inspect variable distributions.

Identified skewed variables and potential outliers.

Performed correlation analysis to assess feature relationships.

Data Preparation

Cleaned missing and inconsistent values.

Encoded categorical features where necessary.

Scaled/normalized numerical variables to prepare for modeling.

Modeling

Selected a classification model appropriate for the data (e.g., Logistic Regression / Decision Tree / Random Forest).

Trained the model and tuned hyperparameters for better performance.

Evaluation

Evaluated model with accuracy, recall, precision, F1-score, and ROC-AUC.

Results showed the model achieved good overall accuracy, with recall highlighted as a key metric due to potential class imbalance.

Deployment (Scenario + Prediction)

Created a real-world scenario requiring prediction from the trained model.

Example: Given a new observation (feature inputs), the model predicts the likelihood of the positive class.

This prediction provides actionable insights for decision-makers (e.g., predicting customer churn ‚Üí who might need retention efforts).

üìä Summary of Results

The exploratory data analysis revealed several skewed variables that benefited from scaling.

The chosen model (e.g., Random Forest) provided balanced performance across evaluation metrics.

Recall was prioritized to minimize false negatives in the business context.

Predictions generated in the example scenario demonstrated how the model can be applied to real-world decision-making.

üôè Acknowledgments

Udacity Data Science Nanodegree program for guidance and project structure.

Scikit-Learn and Python open-source community for tools and libraries.

Stakeholders and dataset providers whose data made this analysis possible.
